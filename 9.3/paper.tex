\documentclass{ansarticle-preprint}
%\usepackage{ucs}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
%\usepackage{cite}
\usepackage{anslistings}
\usepackage{multicol}
\usepackage{pdfsync}

\usepackage{pgfplots}
\usepackage{pgfplotstable}

\usepackage{fontenc}
\usepackage{graphicx}
\usepackage{xspace}

\usepackage{siunitx}

\usepackage{floatflt}

%\renewcommand{\baselinestretch}{2.0}

\usepackage[normalem]{ulem}

\usepackage{todonotes}

\pgfplotsset{compat=1.9}
\definecolor{gnuplot@lightblue}{RGB}{87,181,232}
\definecolor{gnuplot@green}{RGB}{0,158,115}
\definecolor{gnuplot@purple}{RGB}{148,0,212}

\newcommand{\specialword}[1]{\texttt{#1}}
\newcommand{\dealii}{{\specialword{deal.II}}\xspace}
\newcommand{\pfrst}{{\specialword{p4est}}\xspace}
\newcommand{\trilinos}{{\specialword{Trilinos}}\xspace}
\newcommand{\aspect}{\specialword{Aspect}\xspace}
\newcommand{\petsc}{\specialword{PETSc}\xspace}
\newcommand{\cmake}{{\specialword{CMake}}\xspace}
\newcommand{\autoconf}{{\specialword{autoconf}}\xspace}

%
% Author list -- please add yourself in both places below (in
%                alphabetical order) if you think that your
%                contributions to the last release warrant this
%

\hypersetup{
  pdfauthor={
     Daniel Arndt,
     Wolfgang Bangerth,
     Bruno Blais,
     Thomas C. Clevenger,
    Marc Fehling,
    Alexander V. Grayver,
    Timo Heister,
    Luca Heltai,
    Martin Kronbichler,
    Matthias Maier,
    Peter Munch,
    Jean-Paul Pelteret,
    Reza Rastak,
    Ignacio Thomas,
    Bruno Turcksin,
    Zhuoran Wang,
    David Wells
  },
  pdftitle={The deal.II Library, Version 9.2, 2020},
}

\title{The \dealii{} Library, Version 9.2}

 \author[*1]{Daniel Arndt}
 \affil[1]{Computational Engineering and Energy Sciences Group,
   Computational Sciences and Engineering Division,
   Oak Ridge National Laboratory, 1 Bethel Valley Rd.,
   TN 37831, USA.
   \texttt{arndtd/turcksinbr@ornl.gov}}

 \author[2]{Wolfgang~Bangerth}
 \affil[2]{Department of Mathematics and Department of Geosciences, Colorado State University, Fort
   Collins, CO 80523-1874, USA.
   \texttt{bangerth@colostate.edu}}


\author[3]{Bruno Blais}
\affil[3]{Research Unit for Industrial Flows Processes (URPEI), Department of Chemical Engineering,
          Polytechnique Montréal,
          PO Box 6079, Stn Centre-Ville, Montréal, Québec, Canada, H3C 3A7.
          {\texttt{bruno.blais@polymtl.ca}}}

 \author[4]{Thomas~C.~Clevenger}
 \affil[4]{School of Mathematical and Statistical Sciences,
   Clemson University,
   Clemson, SC, 29634, USA
   {\texttt{tcleven/heister@clemson.edu}}}
%
% \author[4]{Denis~Davydov}
% \affil[4]{Chair of Applied Mechanics,
%   Friedrich-Alexander-Universit\"{a}t Erlangen-N\"{u}rnberg,
%   Egerlandstr.\ 5,
%   91058 Erlangen, Germany.
%   {\texttt{\{denis.davydov,jean-paul.pelteret\}@fau.de}}}
%
\author[5]{Marc~Fehling}
\affil[5]{Institute for Advanced Simulation,
  Forschungszentrum J\"{u}lich GmbH,
  52425 J\"{u}lich, Germany.
  {\texttt{m.fehling@fz-juelich.de}}}
%
% \author[6]{Daniel Garcia-Sanchez}
% \affil[6]{Sorbonne Universit\'es, UPMC Univ.\ Paris 06, CNRS-UMR 7588,
%   Institut des NanoSciences de Paris, F-75005, Paris, France
%   {\texttt{daniel.garcia-sanchez@insp.upmc.fr}}}
%
% \author[2]{Graham Harper}
%

\author[6]{Alexander~V.~Grayver}
\affil[6]{Institute of Geophysics,
  ETH Zurich,
  Sonneggstrasse 5, 8092 Z\"{u}rich, Switzerland.
  {\texttt{agrayver@ethz.ch}}}

\author[4]{Timo~Heister}

\author[7]{Luca~Heltai}
\affil[7]{SISSA,
   International School for Advanced Studies,
   Via Bonomea 265,
   34136, Trieste, Italy.
   {\texttt{luca.heltai@sissa.it}}}

 \author[8]{Martin~Kronbichler}
 \affil[8]{Institute for Computational Mechanics,
   Technical University of Munich,
   Boltzmannstr.~15, 85748 Garching, Germany.
   {\texttt{kronbichler/munch@lnm.mw.tum.de}}}

\author[9]{Matthias~Maier}
\affil[9]{Department of Mathematics,
  Texas A\&M University,
  3368 TAMU,
  College Station, TX 77845, USA.
  {\texttt{maier@math.tamu.edu}}}

\author[8,10]{Peter Munch}
 \affil[10]{Institute of Materials Research, Materials Mechanics,
 Helmholtz-Zentrum Geesthacht,
 Max-Planck-Str. 1, 21502 Geesthacht, Germany.
   {\texttt{peter.muench@hzg.de}}}


\author[11]{Jean-Paul~Pelteret}
\affil[11]{Independent researcher.
{\texttt{jppelteret@gmail.com}}}

 \author[12]{Reza Rastak}
 \affil[12]{Department of Civil and Environmental Engineering,
   Stanford University,
   Stanford, CA 94305, USA.
   {\texttt{rastak@stanford.edu}}}

 \author[**13]{Ignacio Tomas}
 \affil[13]{Sandia National Laboratories, Org 1442,
 P.O. Box 5800, MS 1320, Albuquerque, NM, 87185-1320,
   \texttt{itomas@sandia.gov}}

\author[*1]{Bruno~Turcksin}
%

\author[14]{Zhuoran Wang}
\affil[14]{Department of Mathematics, Colorado State University, Fort Collins,
          CO 80523-1874, USA.
{\texttt{zhrwang@math.colostate.edu}}}

\author[15]{David Wells}
\affil[15]{Department of Mathematics, University of North Carolina,
  Chapel Hill, NC 27516, USA.
  {\texttt{drwells@email.unc.edu}}}

\renewcommand{\labelitemi}{--}


\begin{document}
\maketitle

\footnotetext{%
  $^\ast$ This manuscript has been authored by UT-Battelle, LLC under Contract No.
  DE-AC05-00OR22725 with the U.S. Department of Energy.
  %The United States
  %Government retains and the publisher, by accepting the article for
  %publication, acknowledges that the United States Government retains a
  %non-exclusive, paid-up, irrevocable, worldwide license to publish or reproduce
  %the published form of this manuscript, or allow others to do so, for United
  %States Government purposes. The Department of Energy will provide public
  %access to these results of federally sponsored research in accordance with the
  %DOE Public Access Plan (http://energy.gov/downloads/doe-public-access-plan).
}

\footnotetext{%
  $^{**}$ Sandia National Laboratories is a multimission laboratory managed and
  operated by National Technology \& Engineering Solutions of Sandia, LLC, a
  wholly owned subsidiary of Honeywell International Inc., for the U.S.
  Department of Energy's National Nuclear Security Administration under contract
  DE-NA0003525. This document describes objective technical results and analysis.
  Any subjective views or opinions that might be expressed in the paper do not
  necessarily represent the views of the U.S. Department of Energy or the United
  States Government.}

\begin{abstract}
  This paper provides an overview of the new features of the finite element
  library \dealii, version 9.2.
\end{abstract}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Overview}

\dealii{} version 9.2.0 was released May 20, 2020.
This paper provides an
overview of the new features of this release and serves as a citable
reference for the \dealii{} software library version 9.2. \dealii{} is an
object-oriented finite element library used around the world in the
development of finite element solvers. It is available for free under the
GNU Lesser General Public License (LGPL). Downloads are available at
\url{https://www.dealii.org/} and \url{https://github.com/dealii/dealii}.

The major changes of this release are:
%
\begin{itemize}
  \item A new, parallel, fully distributed triangulation class (see
        Section~\ref{subsec:pft});
  \item Substantially improved performance for very large computations
    on tens or hundreds of thousands of processor cores and up to
    trillions of unknowns (see
        Section~\ref{subsec:performance});
  \item Better support for parallel $hp$-adaptive algorithms (see
        Section~\ref{subsec:hp});
  \item Support for particle-based methods (see
        Section~\ref{subsec:particles});
  \item Improved performance of the symbolic differentiation framework (see
        Section~\ref{subsec:symbdiff});
  \item Advances in SIMD capabilities and the matrix-free infrastructure (see
        Section~\ref{subsec:mf});
  \item Advances in GPU support (see
        Section~\ref{subsec:gpu});
  \item Better use of modern C++ language features (see
        Section~\ref{subsec:cxx});
  \item Seven new tutorial programs (see
        Section~\ref{subsec:steps}).
\end{itemize}
%
These major changes are discussed in detail in Section~\ref{sec:major}. There
are a number of other noteworthy changes in the current \dealii{} release
that we briefly outline in the remainder of this section:
%
\begin{itemize}
  \item \dealii{} had decent support for solving complex-valued problems
        for a while already
        (e.g., ones in quantum mechanics -- like the equation used in the
        \texttt{step-58} tutorial program covered below -- or for
        time-harmonic problems). However, there were two
        areas in which support was missing. First, the UMFPACK direct solver
        packaged with \dealii{} did not support solving complex-valued
        linear problems. This has now been addressed: UMFPACK actually can
        solve such systems, we just needed to write the appropriate
        interfaces. Second, the \texttt{DataOut} class that is responsible
        for converting nodal data into information that can then be written
        into files for visualization did not know how to deal with vector-
        and tensor-valued fields whose components are complex numbers. An
        example for this is to solve the time-harmonic version of the
        Maxwell equations that has the electric and magnetic fields as
        solution. This, too, has been addressed in this release.
  \item The new \texttt{DiscreteTime} class provides a more
        consistent, more readable, and less error-prone approach to control
        time-stepping algorithms within time-dependent simulations.
        While providing a rich read-only interface, the non-const interface
        of this class is designed to be minimal
        to enforce a number of important programming invariants, reducing
        the possibility of mistakes in the user code.
        For instance, \texttt{DiscreteTime} ensures that the final time step ends
        precisely on a predefined end time, automatically
        lengthening or shortening the final time step.
  \item A key component of \dealii{} are the \texttt{FEValues} and
        \texttt{FEFaceValues} classes that evaluate finite element functions
        at quadrature points located on cells and faces of a cell,
        respectively~\cite{BangerthHartmannKanschat2007}. This release now
        contains a class \texttt{FEInterfaceValues} that considers the
        restriction of the shape functions from \textit{both} sides of a face and
        allows evaluating jumps and averages of shape functions along this
        face. These are common components of the bilinear forms of
        discontinuous Galerkin schemes (as well as schemes for fourth-order
        equations, see the discussion of \texttt{step-47} below) and greatly
        simplify the implementation of these methods.
  \item Previously, the \texttt{parallel::distributed::ContinuousQuadratureDataTransfer}
        class, which transfers local quadrature point data onto the
        children of newly refined cells, did not allow different cells
        to store quadrature data of different lengths. This release lifts
        this restriction, which enables efficient quadrature data storage and data
        transfer within a triangulation containing multiple material models, each with
        their own number of local state variables and history variables.
        As an example, in a solid mechanics simulation, we can assign a hyper-elastic
        material model to a region of the mesh with no associated
        history variables, while in another part of the triangulation we incorporate an
        elasto-plastic constitutive model, requiring the storage of local plasticity
        data at quadrature points.
        During each refinement of the mesh, we can then use
        \texttt{ContinuousQuadratureDataTransfer} to transfer and interpolate
        plasticity data from parent cells to their children, ignoring the cells
        with the hyper-elastic constitutive model.
\end{itemize}
%
The changelog lists more than 240 other
features and bugfixes.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Major changes to the library}
\label{sec:major}

This release of \dealii{} contains a number of large and significant changes
that will be discussed in this section.
It of course also contains a
vast number of smaller changes and added functionality; the details of these
can be found
\href{https://dealii.org/developer/doxygen/deal.II/changes_between_9_1_1_and_9_2_0.html}{
  in the file that lists all changes for this release}, see \cite{changes92}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{A new fully distributed triangulation class}
\label{subsec:pft}

Previously, all triangulation classes of \dealii{} had in common that the coarse grid is replicated by
all processes in a parallel environment, and the actual mesh used for computations is constructed by repeated
refinement. However, this has its limitations in many
applications where the mesh comes
from an external mesh generator in the form of a file that frequently
already contains millions
or tens of millions of cells. For such configurations, applications might exhaust
available memory already while reading the mesh on each MPI process.

The new \texttt{parallel::fullydistributed::Triangulation} class targets this issue
by distributing also the coarse grid. Such
a triangulation can be created by providing to each process a \texttt{Triangulation\-De\-scrip\-tion::Description} struct, containing
(i) the relevant data to construct the local part of the coarse grid, (ii) the
translation of the local coarse-cell IDs to globally unique IDs, (iii) the hierarchy
of mesh refinement steps, and (iv) the owner of the cells on the active mesh level as well
as on the multigrid levels. For the current release, triangulations set up
this way cannot be adaptively refined after construction, though we plan to
improve this for the next release.

%The \texttt{TriangulationDescription::Description} struct can be filled manually or
%by the utility functions from the \texttt{TriangulationDescription::Utilities}
%namespace. The function \texttt{create\_\allowbreak description\_\allowbreak
%from\_ \allowbreak triangulation()} can convert a base triangulation (partitioned
%serial \texttt{Tri\-angulation} and \texttt{parallel::distributed::Triangulation})
%to such a struct. The advantage of this approach is that all known utility
%functions from the namespaces \texttt{GridIn} and \texttt{GridTools} can be used
%on the base triangulations before converting them to the structs. Since this
%function suffers from the same main memory problems as described above, we also
%provide the function \texttt{create\_description\_from\_triangulation\_in\_groups()},
%which creates the structs only on the master process in a process group. These
%structs are filled one by one and are sent to the relevant processes once they are
%ready. A sensible process group size might contain all processes of one compute node.


The new fully distributed triangulation class supports 1D, 2D, and 3D meshes
including geometric multigrid hierarchies, periodic boundary conditions, and
hanging nodes.

%We intend to extend the usability of the new triangulation class in regard of
%different aspects, e.g., I/O. In addition, we would like to enable adaptive mesh
%refinement, a feature of the other triangulation classes, which is very much
%appreciated by many users. For repartitioning, we plan to use an oracle approach
%known from \texttt{parallel::distributed::Triangulation}. Here, we would like to
%rely on a user-provided partitioner, which might also be a graph partitioner.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Improved large-scale performance}
\label{subsec:performance}

Large-scale simulations with up to 304,128 cores have revealed bottlenecks in release
9.1 during initialization of a number of distributed data structures, due to the usage of expensive collective operations
like \texttt{MPI\_Allgather()} and \texttt{MPI\_\allowbreak
  Alltoall()}. Typical examples are the
pre-computation of the indices of those vector entries (or
other linear index ranges) owned by
each process, which were previously stored in  an array on every process.
This information is needed to set up
the  \texttt{Utilities::MPI::Par\-ti\-ti\-oner} class.
In release 9.2, we have replaced these functions in favor of
consensus algorithms~\cite{hoefler2010scalable}, which can be
found in the namespace \texttt{Utilities::\allowbreak MPI::\allowbreak ConsensusAlgorithms} (short: \texttt{CA}).
Now, only the locally relevant information about the index ranges is
(re)computed when needed, which, for more than 100 MPI processes, uses
point-to-point communications and a single \texttt{MPI\_IBarrier()}.

%Consensus algorithms are algorithms dedicated to efficient dynamic-sparse
%communication patterns. In this context, the term ``dynamic-sparse'' means
%that by the time this function is called, the other processes do not know
%yet that they have to answer requests and
%each process only has to communicate with a small subset of processes of the
%MPI communicator. We provide two flavors of the consensus algorithm: the two-step
%approach \texttt{ConsensusAlgorithms::PEX} and the \texttt{ConsensusAlgorithms::NBX},
%which uses only point-to-point communications and a single \texttt{MPI\_IBarrier()}.
%The class \texttt{ConsensusAlgorithms::Selector} selects one of the two previous
%algorithms, depending on the number of processes.

Users can apply the new algorithms for their own dynamic-sparse problems by
providing a list of target
processes and pack/unpack routines either by implementing the interface
\texttt{CA::\allowbreak Process} or by providing \texttt{std::function}
objects to \texttt{CA::AnonymousProcess}.

By replacing the collective communications during set up and removing the arrays
that contain information for each process (enabled by the application of consensus
algorithms and other modifications---a full list of modifications leading to this
improvement can be found online), we were able to significantly
improve the set up
time for large-scale simulations and to solve a Poisson problem with multigrid
with \num{2.1e12} unknowns on the SuperMUC-NG supercomputer with 304,152
cores~\cite{dealII2020design,Arndt2020sppexa}.
Figure~\ref{fig:init_costs} compares timings of simulations of various problem
sizes (including set up) on 49,152 MPI ranks using a matrix-free
solver~\cite{Arndt2020sppexa,KronbichlerKormann2019,KronbichlerWall2018}; this solver uses discontinuous elements of
degree $5$ in a geometric multigrid (GMG) scheme. The comparison between the previous release 9.1
and the current release 9.2 shows that while the scaling for the V-cycle had been
very good before, many initialization routines have been considerably
improved, especially the enumeration of unknowns on the multigrid levels and
the setup of the multigrid transfer.

\pgfplotstableread{
  dofs  grid  distdofs  mfsetup  mgtransfer  smoother  vcycle
  14155776  0.347457  0.02424084  0.03346427  0.0105426  0.0108817  0.0035033
  28311552  0.35134  0.0215024  0.0340963  0.0095201  0.010616  0.0035887
  56623104  0.41577  0.0243593  0.0432297  0.011557  0.011667  0.0041244
  113246208  0.42172  0.0323919  0.0427806  0.012983  0.011834  0.0043853
  226492416  0.43565  0.0404079  0.0421606  0.012867  0.017305  0.0056340
  452984832  0.5055  0.0500101  0.0562977  0.017739  0.022763  0.0061992
  905969664  0.54552  0.065865  0.0641994  0.019445  0.037431  0.0087932
  1811939328  0.5672  0.104909  0.080807  0.025395  0.063548  0.0133252
  3623878656  0.65541  0.165071  0.129717  0.042294  0.12644  0.0218619
  7247757312  0.74521  0.251417  0.221072  0.073314  0.21807  0.0339264
  14495514624  0.84789  0.403463  0.36894  0.11425  0.4091  0.0669798
  28991029248  0.91983  0.52435  0.5363  0.14189  0.79178  0.1322448
  57982058496  1.0339  0.87448  0.93134  0.23863  1.5896  0.2539445
  115964116992  1.2667  1.46016  1.70806  0.44101  3.1409  0.4956625
  231928233984  1.7597  2.58121  3.3741  0.77624  6.2628  0.9857200
}\tableinit

\pgfplotstableread{
  dofs  grid  distdofs  mfsetup  mgtransfer  smoother  vcycle
  14155776  2.0148  0.9086  0.433886  1.3099  0.010685  0.0035033
  28311552  1.8746  0.91078  0.576777  1.3093  0.010752  0.0035887
  56623104  2.0924  0.98058  0.574183  1.4459  0.011831  0.0041244
  113246208  2.3788  1.22025  0.547492  1.7919  0.011979  0.0043853
  226492416  2.1254  1.50329  0.839219  1.7832  0.017819  0.0056340
  452984832  2.4325  2.10171  0.711708  2.0263  0.023393  0.0061992
  905969664  3.8911  3.58355  1.08478  2.5844  0.045697  0.0087932
  1811939328  4.1803  4.379  0.784804  1.8947  0.061552  0.0133252
  3623878656  2.6654  5.6073  0.833896  1.8912  0.1188  0.0218619
  7247757312  2.7422  7.8463  0.741733  1.8477  0.21291  0.0339264
  14495514624  2.765  9.9848  0.99218  1.8248  0.40665  0.0669798
  28991029248  3.2901  15.0526  1.5124  2.6723  0.81013  0.1322448
  57982058496  3.1927  21.0562  1.72831  2.1998  1.5905  0.2539445
  115964116992  4.1485  31.0087  2.67998  2.9091  3.2009  0.4956625
  231928233984  4.1787  47.195  4.2821  3.9558  6.3362  0.9857200
}\tableinitold

\begin{figure}
  \begin{tikzpicture}
    \begin{loglogaxis}[
        width=0.51\textwidth,
        height=0.38\textwidth,
        xlabel={\# DoFs},
        ylabel={time [s]},
        title={9.1 release},
        xtick={1.4e7,5.7e7,2.3e8,9.1e8,3.6e9,1.4e10,5.8e10,2.3e11},
        xticklabels={1.4$\times 10^7$,5.7$\times 10^7$,2.3$\times 10^8$,9.1$\times 10^8$,3.6$\times 10^9$,1.4$\times 10^{10}$,5.8$\times 10^{10}$,2.3$\times 10^{11}$},
        tick label style={font=\scriptsize},
        xticklabel style={rotate=25,anchor=north east},
        label style={font=\scriptsize},
        legend style={font=\scriptsize},
        %legend to name=legend:scalingMG,
        %legend cell align=left,
        %legend columns = 3,
        ymin=2e-3,ymax=8e1,
        grid, thick]
      \addplot table[x index = 0, y index={1}] {\tableinitold};
      \addplot[red,mark=square*] table[x index = 0, y index={2}] {\tableinitold};
      \addplot[gnuplot@green,mark=otimes] table[x index = 0, y index={3}] {\tableinitold};
      \addplot[gnuplot@lightblue,mark=triangle] table[x index = 0, y index={4}] {\tableinitold};
      \addplot[gnuplot@purple,mark=diamond] table[x index = 0, y index={5}] {\tableinitold};
      \addplot[black,mark=x] table[x index = 0, y index={6}] {\tableinitold};
    \end{loglogaxis}
  \end{tikzpicture}
  \hfill
  \begin{tikzpicture}
    \begin{loglogaxis}[
        width=0.51\textwidth,
        height=0.38\textwidth,
        xlabel={\# DoFs},
        ylabel={time [s]},
        title={9.2 release},
        xtick={1.4e7,5.7e7,2.3e8,9.1e8,3.6e9,1.4e10,5.8e10,2.3e11},
        xticklabels={1.4$\times 10^7$,5.7$\times 10^7$,2.3$\times 10^8$,9.1$\times 10^8$,3.6$\times 10^9$,1.4$\times 10^{10}$,5.8$\times 10^{10}$,2.3$\times 10^{11}$},
        tick label style={font=\scriptsize},
        xticklabel style={rotate=25,anchor=north east}, thick,
        label style={font=\scriptsize},
        legend style={font=\scriptsize},
        legend to name=legend:init,
        legend cell align=left,
        legend columns = 6,
        ymin=2e-3,ymax=8e1,
        grid]
      \addplot table[x index = 0, y index={1}] {\tableinit};
      \addlegendentry{create mesh};
      \addplot[red,mark=square*] table[x index = 0, y index={2}] {\tableinit};
      \addlegendentry{enumerate (mg) dofs};
      \addplot[gnuplot@green,mark=otimes] table[x index = 0, y index={3}] {\tableinit};
      \addlegendentry{init matrix-free};
      \addplot[gnuplot@lightblue,mark=triangle] table[x index = 0, y index={4}] {\tableinit};
      \addlegendentry{init GMG transfer};
      \addplot[gnuplot@purple,mark=diamond] table[x index = 0, y index={5}] {\tableinit};
      \addlegendentry{init smoother};
      \addplot[black,mark=x] table[x index = 0, y index={6}] {\tableinit};
      \addlegendentry{GMG V-cycle};
    \end{loglogaxis}
  \end{tikzpicture}
  \\
  \strut\hfill\pgfplotslegendfromname{legend:init}\hfill\strut
  \caption{\it Comparison of initialization costs of various data structures in the 9.1 release (left) and the new 9.2 release (right) when run on 49,152 MPI ranks.}
  \label{fig:init_costs}
\end{figure}


As part of this effort, we ran benchmarks on the TACC Frontera system, where we were
able to apply the matrix-free geometric multigrid framework to a variable viscosity
Stokes system and achieved weak and strong scaling up to 114K MPI ranks with up to
\num{2.1e11} unknowns. This is likely the largest block system currently solved with
\dealii{} and required various optimizations
and fixes on top of the ones mentioned above:
(i) Bug fixes to concurrent point to point communications.
(ii) Fixes to multigrid transfer with adaptive refinement and more than \num{4e9} unknowns.
(iii) Fixes to index sets in block indices with more than \num{4e9} unknowns.
(iv) Fixes to computations with more than \num{4e9} active cells.
(v) Implementation of IDR(s) solvers to reduce memory overhead.
For more details, see \cite{clevenger_stokes19}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Better support for parallel $hp$-adaptive algorithms}
\label{subsec:hp}

Since the previous release, \dealii{} has had support for $hp$-adaptive finite element methods
on distributed memory systems \cite{dealII91}. We implemented the bare functionality
for $hp$-adaptive methods with the objective to offer the greatest flexibility in
their application. Here, reference finite elements still had to be assigned manually
to each cell, which may not lead to an optimal mesh and is tedious.

With the current release, $hp$-adaptive finite element methods have been further
expanded: New features like decision strategies have been added and the user interface
has been overhauled, effectively making $hp$-methods more attractive to use. We introduced
many new functions that automatize the general workflow for applying $hp$-decision
strategies, which run on top of the previous low-level implementation for both serial
and parallel applications.

The interface is now as simple to use as the one for $h$-adaptive mesh refinement.
Consider the following (incomplete) listing as an example: We estimate both error and
smoothness of the finite element approximation. Further, we flag certain fractions of
cells with the highest and lowest errors for refinement and coarsening, respectively
(here: 30\%/3\%). From those cells listed for adaptation, we designate a subset
for $h$- and $p$-adaptation. The parameters of the corresponding
\texttt{hp::Refinement} function specify the fraction of cells to be $p$-adapted from
those subsets flagged for refinement and coarsening, respectively (here: 90\%/80\%),
while the remaining fraction will be $h$-adapted (here: 10\%/20\%).
\begin{c++}
Vector<float> estimated_error_per_cell(n_active_cells);
KellyErrorEstimator::estimate(
  dof_handler, ..., solution, estimated_error_per_cell, ...);
GridRefinement::refine_and_coarsen_fixed_number(
  triangulation, estimated_error_per_cell, 0.3, 0.03);

Vector<float> estimated_smoothness_per_cell(n_active_cells);
SmoothnessEstimator::Legendre::coefficient_decay(
  ..., dof_handler, solution, estimated_smoothness_per_cell);
hp::Refinement::p_adaptivity_fixed_number(
  dof_handler, estimated_smoothness_per_cell, 0.9, 0.8);
hp::Refinement::choose_p_over_h(dof_handler);

triangulation.execute_coarsening_and_refinement();
\end{c++}

In particular, we implemented decision strategies based on refinement history and
smoothness estimation, and made sure that they work for refinement as well as
coarsening in terms of $h$- and $p$-adaptation in serial and parallel applications.

The former relies on knowing an estimate for the upper error bound \cite[Thm.~3.4]{BabuskaSuri1990}.
For successive refinements, we can predict how the error will change based on
current error estimates and adaptation flags. In the next refinement cycle, these
predicted error estimates allow us to decide whether the choice of adaptation in
the previous cycle was justified, and provide a criterion for the choice in the next
cycle \cite{MelenkWohlmuth2001}.

In general, $p$-refinement is favorable over $h$-refinement in smooth regions of
the finite element approximation \cite[Thm.~3.4]{BabuskaSuri1990}. Thus, estimating
its smoothness provides a suitable decision indicator for $hp$-adaptation. For this
purpose, we express the finite element approximation in an orthogonal
basis of increasing frequency, and consider the decay of their expansion
coefficients as the estimation of smoothness. This has been implemented for both
Fourier coefficients \cite{BangerthKayserHerold2007} and Legendre coefficients
\cite{Mavriplis1994,HoustonSeniorSueli2003,HoustonSueli2005,EibnerMelenk2007}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Support for particle-based methods}
\label{subsec:particles}

Support for particles was originally introduced in \dealii{} version 9.0. These
particles can be used as passive tracers, or as part of more complex
models such as those based on Particle-In-Cells (PIC) approaches \cite{GLHPB18}.

With the current release, support for particles has been further expanded: New parallel
insertion mechanisms and a basic interface to post-process particles have
been added, effectively making their usage more flexible and enabling a larger
range of use cases (such as the immersed boundaries in step-70, see below).

Through the addition of the \texttt{Particles::ParticleHandler::insert\_global\_particles()} member function,
particles can now be inserted in parallel from a vector of points even if these points do not lie
on the subdomain from which the insertion is called. This operation requires extensive
communication between the processes to locate   the MPI process that
owns the cell in which the particle
is located. However, it is made significantly faster  through the usage of bounding boxes
that provide a coarse description of the geometrical shape of each subdomain.
This function also takes care of transferring the properties attached to the particles to their new owner.
This new capability enables particle generators that insert particles at the location
of the support points (\texttt{Particles::Generator::dof\_support\_points()}) and at the quadrature points (\texttt{Particles::Generator::quadrature\_points()}) of a
possibly non-matching triangulation. Consequently, complex particle patterns can be inserted
using unstructured grids generated outside of \dealii{}.

To visualize the motion of particles, the \texttt{Particles::DataOut} class was added to the library.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Improved performance of the symbolic differentiation framework}
\label{subsec:symbdiff}

In the previous release we added support for symbolic expressions, leveraging the
SymEngine library \cite{symengine-web-page}.
Although effective, evaluating lengthy expressions could be a bottleneck as this
was performed using dictionary-based substitution.
We have improved on this by implementing a \texttt{BatchOptimizer} class in the
namespace \texttt{Differentiation::SD} that collects several \texttt{Expression}s
and transforms them in such a way that the equivalent result is returned through
a quicker code path.
This may be done by simply using common subexpression elimination (CSE) for the
dictionary-based expressions, by transformation to a set of nested
\texttt{std::function} objects (the equivalent to \texttt{SymPy}'s ``lambdify'', with or
without using CSE), or by offloading these expressions to the \texttt{LLVM}
just-in-time (JIT) compiler.
Although each of these features is implemented and tested in the SymEngine
library itself, the \texttt{BatchOptimizer} class provides both a uniform
interface to their classes and a convenient interface for scalar expressions,
as well as tensorial expressions formed using the \texttt{deal.II} tensor and
symmetric tensor classes.
It, like the \texttt{Expression} class, is also serializable.

The way the batch optimizer may be employed within a user's code is shown
in the pseudo-code below.
As per usual, one would first define some independent variables, and
subsequently compute some symbolic expressions that are dependent on these
independent variables.
These expression could be, for example, scalar  expressions or tensors of
expressions.
Instead of evaluating these expressions directly, the user would now create an
optimizer to evaluate the dependent functions.
In this example, the selected arithmetic type numerical result will be of type
\texttt{double}, and the \texttt{LLVM} JIT optimizer will be invoked.
It will employ common subexpression elimination and aggressive optimizations
during compilation.
The user then informs the optimizer of all of the independent variables and the
dependent expressions, and invokes the optimization process.
This is an expensive call, as it determines an equivalent code path to evaluate
all of the dependent functions at once.
However, in many cases each evaluation has significantly less computational cost
than evaluating the symbolic expressions directly.
Evaluation is performed when the user constructs a substitution map, giving each
independent variable a numerical representation, and passes those to the
optimizer.
After this step, the numerical equivalent of the individual dependent expressions
may finally be retrieved from the optimizer.

\begin{c++}
using namespace Differentiation::SD;

const Expression x("x");
const Expression y("y");
...
const auto f = calculate_f(x, y, ...); // User function
const auto g = calculate_g(x, y, ...); // User function
...

BatchOptimizer<double> optimizer (OptimizerType::llvm,
                                  OptimizationFlags::optimize_all);
optimizer.register_symbols(x, y, ...);
optimizer.register_functions(f, g, ...);
optimizer.optimize();

const auto substitution_map
  = make_substitution_map({x, ...}, {y, ...}, ...);
optimizer.substitute(substitution_map);

const auto result_f = optimizer.evaluate(f);
const auto result_g = optimizer.evaluate(g);
\end{c++}

This expense of invoking the optimizer may be offset not only by
the number of evaluations performed, but also by the amount of reuse each
instance of a \texttt{BatchOptimizer} has.
In certain circumstances, this can be maximized by generalizing the way in which
the dependent expressions are formulated.
For example, in the context of constitutive modelling the material coefficients
may be made symbolic rather than encoding these into the dependent expressions
as numerical values.
The optimizer may then be used to evaluate an entire family of constitutive laws,
and not a specific one that describes the response of a single material.
Thereafter, serializing the optimizer instance and reloading the contents during
subsequent simulations permits the user to skip the optimization process entirely.
Serialization also enables these complex expressions to be compiled offline.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Advances in SIMD capabilities and the matrix-free infrastructure}
\label{subsec:mf}

%\begin{itemize}
%\item ECL
%\item VectorizedArrayType
%\end{itemize}

The class \texttt{VectorizedArray<Number>} is a key component to achieve
the high node-level performance of the matrix-free algorithms in
\dealii{}~\cite{KronbichlerKormann2012, KronbichlerKormann2019}. It is a
wrapper class around a short vector of $n$ entries of type \texttt{Number}
and maps arithmetic operations to appropriate
single-instruction/multiple-data (SIMD) concepts by intrinsic functions.
The class \texttt{VectorizedArray} has been made more user-friendly in this
release by making it compatible with the STL algorithms found in the header
\texttt{<algorithm>}. The length of the  vector can now be queried by
\texttt{VectorizedArray::size()} and its underlying number type by
\texttt{VectorizedArray::value\_type}. The \texttt{VectorizedArray} class
now supports range-based iteration over its entries. In addition \dealii{}
now supports ternary operations on vectorized data, where for a given
binary comparison operation a true or false value is selected. For example,
the vectorized equivalent of \texttt{(left < right) ? true\_value :
  false\_value} can be expressed as
\begin{c++}
auto result = compare_and_apply_mask<SIMDComparison::less_than>(
left, right, true_value, false_value);
\end{c++}
which compares every element of the vectorized array individually and
selects the corresponding value from the \texttt{true\_value}, or
\texttt{false\_value} array.

In previous \dealii{} releases, the vector length was set at compile time
of the library to match the highest value supported by the given processor
architecture. Now, a second optional template argument can be specified as
\texttt{VectorizedArray<Number, size>}, where \texttt{size} explicitly
controls the vector length within the capabilities of a particular
instruction set. (A full list of supported vector lengths is presented in
Table~\ref{tab:vectorizedarray}.) This allows users to select the vector
length/ISA and, as a consequence, the number of cells to be processed at
once in matrix-free operator evaluations. For example, the \dealii{}-based
library \texttt{hyper.deal}~\cite{munch2020hyperdeal}, which solves the 6D
Vlasov--Poisson equation with high-order discontinuous Galerkin methods
(with more than a thousand degrees of freedom per cell), constructs a
tensor product of two \texttt{MatrixFree} objects of different SIMD-vector
length in the same application and benefits---in terms of performance---by
the possibility of decreasing the number of cells processed by a single
SIMD instruction.


\begin{table}
  \caption{\it Supported vector lengths of the class \texttt{VectorizedArray} and
    the corresponding instruction-set-architecture extensions. }\label{tab:vectorizedarray}
  \centering
  \begin{tabular}{ccc}
    \toprule
    \textbf{double}            & \textbf{float}             & \textbf{ISA}         \\
    \midrule
    VectorizedArray<double, 1> & VectorizedArray<float, 1>  & (auto-vectorization) \\
    VectorizedArray<double, 2> & VectorizedArray<float, 4>  & SSE2/AltiVec         \\
    VectorizedArray<double, 4> & VectorizedArray<float, 8>  & AVX/AVX2             \\
    VectorizedArray<double, 8> & VectorizedArray<float, 16> & AVX-512              \\
    \bottomrule
  \end{tabular}

  \caption{\it Comparison of relevant SIMD-related classes in \dealii{} and \texttt{C++23}.}\label{tab:simd}
  \centering
  \begin{tabular}{cc}
    \toprule
    \textbf{VectorizedArray (\dealii{})} & \textbf{std::simd (\texttt{C++23})}                \\
    \midrule
    VectorizedArray<Number>              & std::experimental::native\_simd<Number>            \\
    VectorizedArray<Number, size>        & std::experimental::fixed\_size\_simd<Number, size> \\ \bottomrule
  \end{tabular}
\end{table}

The new interface of \texttt{VectorizedArray} also enables replacement
by any type with a matching interface. Specifically, this prepares
\dealii{} for the \texttt{std::simd} class that is slated to become
part of the \texttt{C++23} standard.
Table~\ref{tab:simd} compares the \dealii{}-specific SIMD classes and
the equivalent \texttt{C++23} classes. These changes also prepare for specialized
code paths exploiting
vectorization within an element, see~\cite{KronbichlerKormann2019}.

%We welcome the standardization of the SIMD
%parallelization paradigm in C++ and intend to replace step by step our own
%wrapper class, which has been continuously developed over the last decade. We
%would like to emphasize that the work invested in this class was not in vain,
%since many performance-relevant utility functions implemented with \texttt{VectorizedArray} in mind (e.g., \texttt{vectorized\_load\_and\_transpose}
%and \texttt{vectorized\_transpose\_and\_store}) will be still used, since they
%have not become part of the standard.

%Further additions to the \texttt{MatrixFree} infrastructure consist of:
%\begin{itemize}
%\item a new variant of \texttt{MatrixFree::cell\_loop()}: It takes two
%\texttt{std::function} objects with ranges on the locally owned degrees of freedom, one
%with work to be scheduled before the cell operation first touches some
%unknowns and another with work to be executed after the cell operation last
%touches them. The goal of
%these functions is to bring vector operations close to the time when the
%vector entries are accessed by the cell operation, which increases the cache
%hit rate of modern processors by improved temporal locality.
%\item a new form of loop \texttt{MatrixFree::loop\_cell\_centric()}: This
%kind of loop can be used in the context of discontinuous Galerkin methods,
%where both cell and face integrals have to be evaluated. While in the case of
%the traditional \texttt{loop}, cell and face integrals have been performed
%independently, the new loop performs all cell and face integrals of a cell in
%one go. This includes that each face integral has to be evaluated twice, but
%entries have to be written into the solution vector only once with improved
%data locality. Previous publications based on \dealii{} have shown the
%relevance of the latter aspect for reaching higher performance.
%\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Advances in GPU support}
\label{subsec:gpu}

For this release, the most noteworthy improvements in GPU support are the
simplification of the kernel written by user, improvement of error messaging,
and overlapping of computation and communication when using CUDA-aware MPI with
the matrix-free framework.

In order to simplify user code, we now recompute
local degree of freedom and quadrature point indices instead of having the user
keeping track of them. A new option to overlap computation and communication is
now available when using CUDA-aware MPI and matrix-free. The underlying idea is
that only the degrees of freedom (DoFs) on the boundary of the local domain
require communication. Before evaluating the matrix-free operator at these
degrees of freedom, we need to communicate ghost DoFs from other
processors.  Similarly once the operator has been evaluated, we need to update
the resulting global vector with values from other processors. The strategy that
we are now using consists in splitting the DoFs into three groups: one group of DoFs
that are on the local boundary and two groups each owning half of the interior
DoFs. When evaluating the matrix-free operator, we start the MPI communication
to get the ghosted DoFs and evaluate the operator on one of the two interior
DoFs group. When the evaluation is done, we wait for the MPI
communication to be over, and
then evaluate the operator on the boundary DoFs group. When this evaluation is
completed, we start the communication to update the global vector, we evaluate the
operator on the last group of DoFs, and finally wait for the MPI
communication to finish.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Expanded use of C++11 facilities}
\label{subsec:cxx}

Certain types of quantities in a simulation are constants fully known
at compile time. They can be pre-calculated and stored in the
compiled executable in order to avoid unnecessary initialization during
runtime. C++11 and later standards enable such computations by
marking variables and functions with the \texttt{constexpr} keyword.

This optimization is now enabled for the class templates
\texttt{Tensor} and \texttt{SymmetricTensor}.
For instance, the linear mechanical constitutive model for isotropic elastic solids
uses a constant fourth-order elasticity tensor
$\mathbb{C} = \lambda \boldsymbol{I} \otimes \boldsymbol{I} + 2 \mu \mathbb{I}$
which does not depend on the current state of strain.
This tensor can now be statically initialized by defining it as
\texttt{constexpr SymmetricTensor<4, dim>}.
As another example, the lattice vectors in a crystal plasticity model are generally
constant and known during compilation time, enabling their efficient definition as
\texttt{constexpr Tensor<1, dim>}.

Declaring variables, functions, and methods as \texttt{constexpr}
is a C++11 feature that was later expanded by the C++14 standard.
Thus, parts of the \texttt{constexpr} support in \dealii{} depend on the C++
standard supported by the compiler used to install the library.

The next release of \dealii{} will require compiler support for the C++14 standard.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{New and improved tutorial and code gallery programs}
\label{subsec:steps}

Many of the \dealii{} tutorial programs were revised in a variety of
ways as part of this release. A particular example is that we have
converted a number of programs to use range-based for loops (a C++11
feature) for loops over a range of integer indices such as loops over
all quadrature points or all indices of degrees of freedom during
assembly. This makes sense given that the
range-based way of writing loops seems to be the idiomatic approach
these days, and that we had previously already converted loops over
all cells in this way.

In addition, there are a number of new tutorial programs:
\begin{itemize}
  \item \texttt{step-47} is a new program that solves the biharmonic
        equation $\Delta^2 u = f$ with
        ``clamped'' boundary condition given by $u=g, \partial u/\partial
          n=h$. This program is based on the $C^0$ interior
        penalty ($C^0$IP) method for fourth order problems
        \cite{Brenner2005}. In order to overcome
        shortcomings of classical approaches, this method uses $C^0$ Lagrange finite
        elements and introduces ``jump'' and ``average'' operators on
        interfaces of elements that penalize the jump of the gradient of the
        solution in order to obtain convergence to the $H^2$-regular
        solution of the equation.

        The $C^0$IP approach is a modern alternative to classical methods that
        use $C^1$-conforming elements such as the Argyris
        element, the Clough-Tocher element and others, all developed in the
        late 1960s. From a twenty-first century perspective, they can only be
        described as bizarre in their construction. They are also exceedingly
        cumbersome to implement if one wants to use general meshes. As a
        consequence, they have largely fallen out of favor and \dealii{} currently
        does not contain implementations of these shape functions.

  \item \texttt{step-50} is a program that demonstrates the parallel geometric multigrid features
  in \dealii{} as described in \cite{ClevengerHeisterKanschatKronbichler2019}.
  The problem considered is a variable viscosity Laplace equation and it is solved
  with three different approaches:
  (i) using a matrix-based geometric multigrid based on Trilinos or
  PETSc; (ii) using a matrix-free
  geometric multigrid; (iii) using algebraic multigrid (Trilinos ML). The tutorial
  demonstrates the superiority of the matrix-free method for the problem under consideration,
  and shows that, for matrix-based formulations, the performance of algebraic and
  geometric multigrid methods are roughly comparable.


  \item \texttt{step-58} is a program that solves the nonlinear
        Schr{\"o}dinger equation, which in non-dimensional form reads
        \begin{align*}
          - i \frac{\partial \psi}{\partial t}
          - \frac 12 \Delta \psi
          + V \psi
          + \kappa |\psi|^2 \psi
           & = 0,
        \end{align*}
        augmented by appropriate initial and boundary conditions and using
        an appropriate form for the potential $V=V(\mathbf x)$. The
        tutorial program focuses on two specific aspects for which this
        equation serves as an excellent test case: (i) Solving
        complex-valued problems without splitting the equation into its
        real and imaginary parts (as \texttt{step-29} does, for
        example). (ii)~Using operator splitting techniques. The equation is
        a particularly good test case for this technique because the only
        nonlinear term, $\kappa |\psi|^2 \psi$, does not contain any
        derivatives and consequently forms an ODE at each node point, to be
        solved in each time
        step in an operator splitting scheme (for which, furthermore, there
        exists an analytic solution), whereas the remainder of the
        equation is linear and easily solved using standard finite element
        techniques.

  \item \texttt{step-65} presents \texttt{TransfiniteInterpolationManifold}, a
        manifold class that can propagate curved boundary information into the
        interior of a computational domain by transfinite interpolation \cite{Gordon82}.
        This manifold is a prototype for many other manifolds in that it is relatively
        expensive to compute the new points, especially for higher order mappings. Since
        typical programs query higher-order geometries in a large variety of contexts,
        the contribution of the mapping to the run time can be significant. As a
        solution, the tutorial also presents the class \texttt{MappingQCache}, which
        samples the information of expensive manifolds in the points of a
        \texttt{MappingQ} and caches it. The tutorial shows that this makes all queries
        to the geometry very cheap.

  \item \texttt{step-67} is an explicit time integrator for the
        compressible Euler equations discretized with a high-order discontinuous
        Galerkin (DG) scheme using the matrix-free infrastructure. Besides the use of
        matrix-free evaluators for systems of equations and over-integration, it also
        presents \texttt{MatrixFreeOperators::CellwiseInverseMassMatrix}, a fast implementation
        of the action of the inverse mass matrix in the DG setting using tensor
        products. Furthermore, this tutorial demonstrates the usage of new
        pre and post operations, which can be passed to
        \texttt{MatrixFree::cell\_loop()}, to schedule operations on sections of vectors close
        to the matrix-vector product to increase data locality.

  \item \texttt{step-69} presents a first-order scheme solving the compressible
        Euler equations of gas dynamics with a graph-viscosity stabilization
        technique. Beside the usual conservation properties of mass, momentum, and
        total energy, the method also guarantees that the constructed solution
        obeys pointwise stability constraints (in particular positivity of
        density and internal
        energy, and a local minimum principle on the specific entropy). As such
        \text{step-69} is strictly speaking more a collocation-type discretization
        than a variational formulation, even though it is implemented with finite
        elements.

        The time-update at each node requires the evaluation of a right-hand side
        that depends (nonlinearly) on information from the previous time-step that
        spans more than one cell. Therefore, assembly loops operate directly on the
        sparsity graph in order to retrieve information from the entire stencil
        associated with each node. From a programming perspective, \texttt{step-69}
        features a number of techniques that are of interest for a wider audience:
        It discusses a hybrid thread and MPI parallelized scheme with efficient
        MPI node-local numbering of degrees of freedom. It showcases how to
        perform asynchronous write-out of results using a background thread with
        \texttt{std::async}, and discusses a simple but effective checkpointing and
        restart technique.

  \item \texttt{step-70} solves a fluid structure interaction problem on
        non-matching parallel distributed triangulations, showing the usage of
        the \texttt{particles::ParticleHandler} class for two different tasks:
        (i) to track the position of quadrature points of a non-matching
        solid grid immersed in a fluid grid, and (ii) to track the position of a
        collection of massless tracers.

        The program considers a mixing problem in the laminar flow
        regime. Such problems occur in a wide range of applications ranging
        from chemical engineering to power generation (e.g. turbomachinery).
        Mixing problems are particularly hard to solve numerically, because
        they often involve a container (with fixed boundaries, and possibly
        complex geometries such as baffles), and one (or more) immersed and
        rotating impellers, making it difficult to use Arbitrary Lagrangian
        Eulerian formulations, where the fluid domain – along with the mesh! –
        is smoothly deformed to follow the deformations of the immersed solid.
        We show how to solve such problems using a penalization technique
        where the flow problem is solved in the union of the fluid and solid
        domain, and the movement of the solid is imposed weakly using a
        Nitsche-like penalization.

        The \texttt{particles::ParticleHandler} class is used in this context
        to allow integration of the fluid basis functions on the solid domain
        (which is immersed and non-matching with regard to the fluid grid). This is
        achieved by attaching the information required to perform the
        integration on the solid grid, to the property field of the particles
        associated with the quadrature points of the solid, and advecting those
        particles according to the solid velocity. In this program, we
        externally prescribe rather than solve for the
        the solid velocity; however,
        the program can easily be extended to allow for the solution of
        coupled elasticity equations on the solid domain.
\end{itemize}

There are also new programs in the code gallery (a collection of
user-contributed programs that often solve more complicated problems
than tutorial programs, and intended as starting points for further
research rather than as teaching tools):
\begin{itemize}
  \item A program based on the XBraids library to solve time-dependent
    problems in a parallel-in-time fashion. This program was
    contributed by Joshua Christopher.
  \item A program that solves the biphasic nonlinear
    poro-viscoelasticity equations based on Ogden hyperelasticity, to
    explore the porous and viscous contributions in brain mechanics
    applications.  This program was contributed by Ester Comellas and
    Jean-Paul Pelteret. The program also serves as a demonstration of
    the automatic differentiation capabilities of \dealii{}.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Python interfaces}
\label{subsec:python}

Initial support for Python has existed in \dealii{} since version
9.0. The present release significantly extends the Python
interface. Specifically, a large number of methods from classes such
as \texttt{Triangulation, CellAccessor, TriaAccessor, Mapping,
  Manifold, GridTools} can now be invoked from Python. We have focused
on methods and functions that are widely used when a mesh is created
and parameters related to the boundary, manifold, and material
identifiers are assigned. The following listing gives an idea of how
such code looks:
\begin{python}
import PyDealII.Release as dealii

triangulation = dealii.Triangulation('2D')
triangulation.generate_hyper_shell(center = dealii.Point([0, 0]),
                                   inner_radius = 0.5, outer_radius = 1.,
                                   n_cells = 0, colorize = True)

triangulation.refine_global(2)

for cell in triangulation.active_cells():
    cell.material_id = 1 if cell.center().x > 0 else 2

    for face in cell.faces():
        if face.at_boundary() and face.boundary_id == 1:
            cell.refine_flag = 'isotropic'

triangulation.execute_coarsening_and_refinement()
\end{python}
The mesh that results from this code is shown in Fig.~\ref{fig:pymesh}.

\begin{floatingfigure}[r]{0.3\textwidth}
  \centering
  \vspace*{-1.4cm}
  \includegraphics[width=0.3\textwidth]{python_mesh.png}
  \vspace*{-5mm}
  \caption{\it The mesh generated by the Python code shown in the main
    text. Cells are colored by material id.}
  \vspace*{0.2cm}
  \label{fig:pymesh}
\end{floatingfigure}
All triangulations created from within Python are serial. However,
once the mesh is designed, the triangulation can be serialized along
with the auxiliary information about possible refinement, boundaries,
materials and manifolds. This object can be easily deserialized within
a C++ program for subsequent production runs. Furthermore, such a
serialized triangulation can also be used in the construction of
\texttt{parallel::shared}, \texttt{parallel::distributed}, and
\texttt{parallel::fullydistributed} triangulations (see also Section~\ref{subsec:pft}).

To facilitate the illustration of the new Python bindings, tutorial programs \href{https://github.com/dealii/dealii/blob/dealii-9.2/examples/step-49/step-49.ipynb}{step-49} and \href{https://github.com/dealii/dealii/blob/dealii-9.2/examples/step-53/step-53.ipynb}{step-53} were replicated as Jupyter notebooks.

The introspective nature of the Python language makes it easy to infer
the list of supported methods from the Python objects, for example by
typing \texttt{dir(PyDealII.Release.Triangulation)}.
The current Python interface does not yet provide access to \dealii{}'s finite element machinery, i.e., classes such as \texttt{DoFHandler, FE\_*, FEValues}, etc.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Incompatible changes}

The 9.2 release includes
\href{https://dealii.org/developer/doxygen/deal.II/changes_between_9_1_1_and_9_2_0.html}
{around 60 incompatible changes}; see \cite{changes92}. The majority of these changes
should not be visible to typical user codes; some remove previously
deprecated classes and functions; and the majority change internal
interfaces that are not usually used in external
applications. That said, the following are worth mentioning since they
may have been more widely used:
\begin{itemize}
  \item Two functions that provide information about all processes,
        namely
        \begin{itemize}
          \item \texttt{DoFHandler::locally\_owned\_dofs\_per\_processor()}
          \item \texttt{DoFHandler::locally\_owned\_mg\_dofs\_per\_processor()}
        \end{itemize}
        have been deprecated. As discussed in
        Subsection~\ref{subsec:performance}, \dealii{} by default no longer
        stores information for all processes on all processes, but only
        local or locally-relevant information. On the other
        hand, if necessary, global information can still be computed using,
        for example, calling
        \texttt{Utilities::MPI::Allgather(locally\_owned\_info(), comm)}.

  \item A corresponding change has been made in parallel triangulation
        classes:
        \texttt{parallel::} \texttt{TriangulationBase::compute\_n\_locally\_owned\_active\_cells\_per\_processor()}
        can be used to obtain information about how many cells each
        process owns.
\end{itemize}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{How to cite \dealii}\label{sec:cite}

In order to justify the work the developers of \dealii{} put into this
software, we ask that papers using the library reference one of the
\dealii{} papers. This helps us justify the effort we put into it.

There are various ways to reference \dealii{}. To acknowledge the use of
the current version of the library, \textbf{please reference the present
  document}. For up to date information and a bibtex entry
see
\begin{center}
  \url{https://www.dealii.org/publications.html}
\end{center}

The original \dealii{} paper containing an overview of its
architecture is \cite{BangerthHartmannKanschat2007}. If you rely on
specific features of the library, please consider citing any of the
following:
\begin{multicols}{2}
  \vspace*{-36pt}
  \begin{itemize}
    \item For geometric multigrid: \cite{Kanschat2004,JanssenKanschat2011,ClevengerHeisterKanschatKronbichler2019};
    \item For distributed parallel computing: \cite{BangerthBursteddeHeisterKronbichler11};
    \item For $hp$~adaptivity: \cite{BangerthKayserHerold2007};
    \item For partition-of-unity (PUM) and enrichment methods of the
          finite element space: \cite{Davydov2016};
    \item For matrix-free and fast assembly techniques:
          \cite{KronbichlerKormann2012,KronbichlerKormann2019};
    \item For computations on lower-dimensional manifolds:
          \cite{DeSimoneHeltaiManigrasso2009};
    \item For curved geometry representations and manifolds:
          \cite{HeltaiBangerthKronbichlerMola2019};
          \vfill\null  \columnbreak
    \item For integration with CAD files and tools:
          \cite{HeltaiMola2015};
    \item For boundary element computations:
          \cite{GiulianiMolaHeltai-2018-a};
    \item For \texttt{LinearOperator} and \texttt{PackagedOperation} facilities:
          \cite{MaierBardelloniHeltai-2016-a,MaierBardelloniHeltai-2016-b};
    \item For uses of the \texttt{WorkStream} interface:
          \cite{TKB16};
    \item For uses of the \texttt{ParameterAcceptor} concept, the
          \texttt{MeshWorker::ScratchData} base class, and the
          \texttt{ParsedConvergenceTable} class:
          \cite{SartoriGiulianiBardelloni-2018-a};
    \item For uses of the particle functionality in \dealii{}:
          \cite{GLHPB18}.
          \vfill\null
  \end{itemize}
\end{multicols}

\dealii{} can interface with many other libraries:
\begin{multicols}{3}
  \begin{itemize}
    \item ADOL-C \cite{Griewank1996a,adol-c}
    \item ARPACK \cite{arpack}
    \item Assimp \cite{assimp}
    \item BLAS and LAPACK \cite{lapack}
    \item cuSOLVER \cite{cusolver}
    \item cuSPARSE \cite{cusparse}
    \item Gmsh \cite{geuzaine2009gmsh}
    \item GSL \cite{gsl2016}
    \item Ginkgo \cite{ginkgo-web-page}
    \item HDF5 \cite{hdf5}
    \item METIS \cite{karypis1998fast}
    \item MUMPS \cite{ADE00,MUMPS:1,MUMPS:2,mumps-web-page}
    \item muparser \cite{muparser-web-page}
    \item nanoflann \cite{nanoflann}%
          %
          % nanoflann has been deprecated and will be removed for 9.3. remove
          % this line in the 9.3 release paper
          %
    \item NetCDF \cite{rew1990netcdf}
          %
          % netcdf has been deprecated and will be removed for 9.3. remove
          % this line in the 9.3 release paper
          %
    \item OpenCASCADE \cite{opencascade-web-page}
    \item p4est \cite{p4est}
    \item PETSc \cite{petsc-user-ref,petsc-web-page}
    \item ROL \cite{ridzal2014rapid}
    \item ScaLAPACK \cite{slug}
    \item SLEPc \cite{Hernandez:2005:SSF}
    \item SUNDIALS \cite{sundials}
    \item SymEngine \cite{symengine-web-page}
    \item TBB \cite{Rei07}
    \item Trilinos \cite{trilinos,trilinos-web-page}
    \item UMFPACK \cite{umfpack}
  \end{itemize}
\end{multicols}
Please consider citing the appropriate references if you use
interfaces to these libraries. We note that the nanoflann and NetCDF
interfaces are now deprecated and will be removed in \dealii{} version
9.3.

The two previous releases of \dealii{} can be cited as
\cite{dealII90,dealII91}.


\section{Acknowledgments}

\dealii{} is a world-wide project with dozens of contributors around the
globe. Other than the authors of this paper, the following people
contributed code to this release:\\
%
% Uwe Koecher doesn't usually show up in the changelog, but
% we should make sure he's listed.
%
% 9.2: updated 5/11/2020 MM
Pasquale Africa,
Ashna Aggarwal,
Giovanni Alzetta,
Mathias Anselmann,
Kirana Bergstrom,
Manaswinee Bezbaruah,
Benjamin Brands,
Yong-Yong Cai,
Fabian Castelli,
Joshua Christopher,
Ester Comellas,
Katherine Cosburn,
Denis Davydov,
Elias Dejene,
Stefano Dominici,
Brett Dong,
Luel Emishaw,
Niklas Fehn,
Isuru Fernando,
Rebecca Fildes,
Menno Fraters,
Andres Galindo,
Daniel Garcia-Sanchez,
Rene Gassmoeller,
Melanie Gerault,
Nicola Giuliani,
Brandon Gleeson,
Anne Glerum,
Krishnakumar Gopalakrishnan,
Graham Harper,
Mohammed Hassan,
Nicole Hayes,
Bang He,
Johannes Heinz,
Jiuhua Hu,
Lise-Marie Imbert-Gerard,
Manu Jayadharan,
Daniel Jodlbauer,
Marie Kajan,
Guido Kanschat,
Alexander Knieps,
Uwe K{\"o}cher,
Paras Kumar,
Konstantin Ladutenko,
Charu Lata,
Adam Lee,
Wenyu Lei,
Katrin Mang,
Mae Markowski,
Franco Milicchio,
Adriana Morales Miranda,
Bob Myhill,
Emily Novak,
Omotayo Omosebi,
Alexey Ozeritskiy,
Rebecca Pereira,
Geneva Porter,
Laura Prieto Saavedra,
Roland Richter,
Jonathan Robey,
Irabiel Romero,
Matthew Russell,
Tonatiuh Sanchez-Vizuet,
Natasha S. Sharma,
Doug Shi-Dong,
Konrad Simon,
Stephanie Sparks,
Sebastian Stark,
Simon Sticko,
Jan Philipp Thiele,
Jihuan Tian,
Sara Tro,
Ferdinand Vanmaele,
Michal Wichrowski,
Julius Witte,
Winnifried Wollner,
Ming Yang,
Mario Zepeda Aguilar,
Wenjuan Zhang,
Victor Zheng.

Their contributions are much appreciated!


\bigskip

\dealii{} and its developers are financially supported through a
variety of funding sources:

D.~Arndt and B.~Turcksin: Research sponsored by the Laboratory Directed Research and
Development Program of Oak Ridge National Laboratory, managed by UT-Battelle,
LLC, for the U. S. Department of Energy.

W.~Bangerth, T.~C.~Clevenger, and T.~Heister were partially
supported by the Computational Infrastructure
in Geodynamics initiative (CIG), through the National Science
Foundation under Award No.~EAR-1550901 and The
University of California -- Davis.


W.~Bangerth was also partially supported by award OAC-1835673 as part of the Cyberinfrastructure for Sustained Scientific Innovation (CSSI)
program, DMS-1821210,
and EAR-1925595.

%D.~Davydov was supported by the German Research Foundation (DFG), grant DA
%1664/2-1 and the Bayerisches Kompetenznetzwerk
%f\"ur Technisch-Wissenschaftliches Hoch- und H\"ochstleistungsrechnen
%(KONWIHR).

B.~Blais was partially supported by the National Science and Engineering Research Council of Canada(NSERC)  through the RGPIN-2020-04510 Discovery Grant

T.~C.~Clevenger was also partially supported EAR-1925575 and OAC-2015848.

A.~V.~Grayver was partially supported by the European Space Agency
Swarm DISC program.

Timo Heister was also partially supported by the National Science Foundation (NSF)
Award DMS-2028346, OAC-2015848, EAR-1925575, and by
Technical Data Analysis, Inc. through US Navy STTR Contract N68335-18-C-0011.

L.~Heltai was partially supported by the Italian Ministry of Instruction,
University and Research (MIUR), under the 2017 PRIN project NA-FROM-PDEs MIUR
PE1, ``Numerical Analysis for Full and Reduced Order Methods for the efficient
and accurate solution of complex systems governed by Partial Differential
Equations''.

M.~Kronbichler was supported by the German
Research Foundation (DFG) under the project ``High-order discontinuous
Galerkin for the exa-scale'' (\mbox{ExaDG}) within the priority program ``Software
for Exascale Computing'' (SPPEXA) and the Bayerisches Kompetenznetzwerk
f\"ur Technisch-Wissen\-schaft\-li\-ches Hoch- und H\"ochstleistungsrechnen
(KONWIHR) in the context of the project
``Performance tuning of high-order discontinuous Galerkin solvers for
SuperMUC-NG''.

M.~Maier was partially supported by ARO MURI Award No. W911NF-14-0247 and
NSF Award DMS-1912847.

D.~Wells was supported by the National Science Foundation (NSF) through Grant
DMS-1344962.

Z.~Wang was partially
supported by the National Science Foundation under award OAC-1835673.

The Interdisciplinary Center for Scientific Computing (IWR) at Heidelberg
University has provided hosting services for the \dealii{} web page.


The authors acknowledge the Texas Advanced Computing Center (TACC) at The
University of Texas at Austin for providing access to HPC resources that have
contributed to the research results reported within this paper.

Clemson University is acknowledged for generous allotment of compute time on
the Palmetto cluster.


\bibliography{paper}{}
\bibliographystyle{abbrv}

\end{document}
